{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce the ablation study table, kmeans agreement score table and flow prediction performance boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tikzplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.figsize'] = [6.5, 4.4]\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "plt.rcParams['font.family'] = 'serif' \n",
    "default_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_type = 'multimodal'\n",
    "\n",
    "assert synthetic_type == 'unimodal' or synthetic_type == 'multimodal'\n",
    "\n",
    "figures_save_folder = f\"figures/ablation_{synthetic_type}/\"\n",
    "tables_save_folder = f\"tables/ablation_{synthetic_type}/\"\n",
    "os.makedirs(figures_save_folder, exist_ok=True)\n",
    "os.makedirs(tables_save_folder, exist_ok=True)\n",
    "\n",
    "# compare_inx = 5 if synthetic_type == 'unimodal' else 2\n",
    "\n",
    "model_dict = {\"dnn2_engi_feat\": 'f.e.+\\ndnn2', \n",
    "              \"dnn2_node2vec\": 'n2v+\\ndnn2',\n",
    "              \"dnn2_both\": 'both+\\ndnn2',\n",
    "              'gated': 'gated', \n",
    "              'grad': 'grad',\n",
    "              'fairness_goodness': \"Kumar\\net.al.\"}\n",
    "\n",
    "repr_dict = {\"dnn2_engi_feat\": 'engi. feat.', \n",
    "             \"dnn2_node2vec\": 'node2vec', \n",
    "             \"dnn2_both\": 'both',\n",
    "             'gated': 'gated', \n",
    "             'grad': 'grad'}\n",
    "\n",
    "model_dict_wo_enter = {wenter: wenter.replace(\"+\\n\", \"+\").replace(\"\\n\", \" \") for wenter in model_dict.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(graph_name, synthetic_type):\n",
    "    RESULTS_TMPL = \"results/{}_{}/\"\n",
    "    BASELINE_TMPL = \"{}_ablation_baseline_results.csv\"\n",
    "    GRAD_TMPL = \"{}_ablation_grad_baseline_results.csv\"\n",
    "    GATED_GRAD_TMPL = \"{}_ablation_joint_results.csv\"\n",
    "    FAIRNESS_GOODNESS_TMPL = \"{}_ablation_fairness_goodness.csv\"\n",
    "    INIT_TMPL = \"{}_ablation_init_results.csv\"\n",
    "    FLOW_INFO_TMPL = \"flow_info.csv\"\n",
    "    folder = RESULTS_TMPL.format(graph_name, synthetic_type)\n",
    "    \n",
    "    results = dict()\n",
    "    \n",
    "    results[\"gated\"] = pd.read_csv(os.path.join(\n",
    "        folder, GATED_GRAD_TMPL.format(graph_name)\n",
    "    ))\n",
    "    \n",
    "    results[\"grad\"] = pd.read_csv(os.path.join(\n",
    "        folder, GRAD_TMPL.format(graph_name)\n",
    "    ))\n",
    "    \n",
    "    baseline_df = pd.read_csv(os.path.join(\n",
    "        folder, BASELINE_TMPL.format(graph_name)\n",
    "    ))\n",
    "    unique_baselines = baseline_df.baseline_name.unique()\n",
    "    for bl in unique_baselines:\n",
    "        results[bl] = baseline_df.loc[baseline_df.baseline_name == bl, :]\n",
    "    \n",
    "\n",
    "    \n",
    "    results[\"fairness_goodness\"] = pd.read_csv(os.path.join(\n",
    "        folder, FAIRNESS_GOODNESS_TMPL.format(graph_name)\n",
    "    ))\n",
    "\n",
    "\n",
    "    results[\"init\"] = pd.read_csv(os.path.join(\n",
    "        folder, INIT_TMPL.format(graph_name)\n",
    "    ))\n",
    "    \n",
    "\n",
    "    results[\"flow_info\"] = pd.read_csv(os.path.join(\n",
    "        folder, FLOW_INFO_TMPL\n",
    "    ))\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_models_repr_columns(results):\n",
    "    for graph_name, results_ in results.items():\n",
    "        for key, df in results_.items():\n",
    "            df['models'] = model_dict.get(key, key)\n",
    "            df['representations'] = repr_dict.get(key, key)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_abl_inx(results, column=\"val_median_mag_error\", select=\"min\"):\n",
    "    abl_inx_dict = dict()\n",
    "    for graph_name, results_ in results.items():\n",
    "        abl_inx_dict[graph_name] = dict()\n",
    "        for key, df in results_.items():\n",
    "            if \"ablation_idx\" not in df.columns:\n",
    "                continue\n",
    "            grouped = df.groupby(\"ablation_idx\").agg({column: \"mean\"})\n",
    "            abl_idx = grouped.idxmin().item()\n",
    "            abl_inx_dict[graph_name][key] = abl_idx\n",
    "    return abl_inx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_names = [ \"cora\", \"bitcoin\", \"complete\"]\n",
    "selected_models = [ 'gated', 'grad', \"dnn2_engi_feat\", \"dnn2_node2vec\", \"fairness_goodness\"]\n",
    "results = dict()\n",
    "for graph_name in graph_names:\n",
    "    results[graph_name] = load_results(graph_name, synthetic_type)\n",
    "results = add_models_repr_columns(results)\n",
    "abl_inx_dict = get_best_abl_inx(results)\n",
    "\n",
    "best_results = dict()\n",
    "for graph_name in graph_names:\n",
    "    best_results[graph_name] = []\n",
    "    for key, result in results[graph_name].items():\n",
    "        if key not in selected_models:\n",
    "            continue\n",
    "        best_results[graph_name].append(result.loc[(result['ablation_idx'] == abl_inx_dict[graph_name][key]), :])\n",
    "    best_results[graph_name] = pd.concat(best_results[graph_name], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create ablation study table\n",
    "def make_init_improvement_table(grad_results, col='median_mag_error', new_name='error*'):\n",
    "    \n",
    "    columns = [f'train_{col}', f'val_{col}']\n",
    "    names = [f'train {new_name}', f'val {new_name}']\n",
    "    name_mapper = dict(zip(columns, names))\n",
    "    name_mapper['ablation_idx'] = 'init and reg.'\n",
    "    \n",
    "    \n",
    "\n",
    "    df_results = grad_results.loc[:, [f'train_{col}', f'val_{col}', 'ablation_idx']]\n",
    "    df_results['ablation_idx'] = df_results['ablation_idx'].map({0: 'normal noise', \n",
    "                                                                 1: 'LSQR', \n",
    "                                                                 2: 'LSQR+',\n",
    "                                                                 3: 'LSQR+, L1($u$)',\n",
    "                                                                 4: 'LSQR+, L1($z$)',\n",
    "                                                                 5: 'LSQR+, L1($u$), L1($z$)'\n",
    "                                                                })\n",
    "    \n",
    "    \n",
    "    all_res_df = df_results.rename(name_mapper, axis='columns')\n",
    "    \n",
    "    out = all_res_df.groupby(['init and reg.'], sort=False).agg({\n",
    "        f'val {new_name}': ['mean', 'std'],\n",
    "        f'train {new_name}': ['mean', 'std']})\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create train and validation box plot\n",
    "def make_comparison(best_results, score='median_mag_error', split=\"val\"):\n",
    "    \n",
    "    res = best_results.loc[:, ['models']]\n",
    "    res[score] = best_results[f'{split}_{score}']\n",
    "    \n",
    "#     res['models'] = ['models'].map\n",
    "    \n",
    "    return res\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kmeans_agreement_score_comparison(best_results, score='multimodal_score'):\n",
    "    res_val = best_results.loc[:, ['representations']]\n",
    "    res_val[score] = best_results[score]\n",
    "    res_val = res_val.rename({'multimodal_score': 'k-means agreement'}, axis='columns')\n",
    "    out = res_val.groupby('representations', sort=False).agg({'k-means agreement': ['mean', 'std']})\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To a dense comparison of training and validation results for all models\n",
    "def setup_df_for_ablation_plot_all_train_val(results_dict, score='loss', remove_index=False):\n",
    "    for key, result in results_dict.items():\n",
    "        if key == 'baseline' or key == 'baselines':\n",
    "            result['models'] = result['baseline_name']\n",
    "        else:\n",
    "            result['models'] = key\n",
    "    all_res = pd.concat(list(results_dict.values()))\n",
    "\n",
    "    res_val = all_res.loc[:, ['models', 'ablation_idx']]\n",
    "    res_val[score] = all_res[f'val_{score}']\n",
    "    res_val['split'] = 'val'\n",
    "\n",
    "    res_train = all_res.loc[:, ['models', 'ablation_idx']]\n",
    "    res_train[score] = all_res[f'train_{score}']\n",
    "    res_train['split'] = 'train'\n",
    "\n",
    "    split_res = pd.concat((res_train, res_val))\n",
    "    if remove_index:\n",
    "        split_res = remove_abl_inx(split_res, remove_index)\n",
    "    return split_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_improvement(results, graph_name):\n",
    "    gated_init_improvment = make_init_improvement_table(results[\"gated\"])\n",
    "    gated_init_improvment.to_latex(os.path.join(tables_save_folder, f\"{graph_name}_init_improvement.tex\"), \n",
    "                                        escape=False, float_format=\"%.2f\")\n",
    "    print(gated_init_improvment)\n",
    "\n",
    "def agreement_score(results, graph_name):\n",
    "    if synthetic_type != 'multimodal':\n",
    "        print(\"agreement score only for mulimodal\")\n",
    "        return\n",
    "    out = make_kmeans_agreement_score_comparison(results)\n",
    "    out.to_latex(os.path.join(tables_save_folder, f\"{graph_name}_kmeans_agreement.tex\"), escape=False, float_format=\"%.2f\")\n",
    "    print(out)\n",
    "    \n",
    "def plot_boxes(results, graph_name, split):\n",
    "#     score = 'median_mag_error'\n",
    "#     score_name=r'log$_{10}$ median rel. error'\n",
    "    score = 'MeAE'\n",
    "    score_name='median abs. error'\n",
    "    data = make_comparison(results, score=score, split=split)\n",
    "    g = sns.catplot(x=\"models\", y=score,\n",
    "                    data=data, \n",
    "                    kind='box', \n",
    "                    linewidth=1.5)\n",
    "    g.set(ylabel=score_name)\n",
    "    sns.despine(left=True)\n",
    "    xticklabels = list(x._text for x in g_val._axes[0][0].get_xticklabels())\n",
    "    xticklabels_latex =  list(x.replace(\"\\n\", \"\\\\\\\\\") for x in xticklabels)\n",
    "    g.set_xticklabels(xticklabels_latex)\n",
    "    tikzplotlib.save(os.path.join(figures_save_folder, f\"{graph_name}_{split}_{score}.tex\"),\n",
    "                    extra_axis_parameters=[\"xticklabel style={align=center}\"])\n",
    "    g.set_xticklabels(xticklabels)\n",
    "    g.fig.savefig(os.path.join(figures_save_folder, f\"{graph_name}_{split}_{score}.pdf\"), bbox_inches='tight')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        val error*           train error*          \n",
      "                              mean       std         mean       std\n",
      "init and reg.                                                      \n",
      "normal noise             -0.042623  0.003290    -1.234667  0.014708\n",
      "LSQR                     -0.042221  0.002488    -0.449679  0.010975\n",
      "LSQR+                    -0.032853  0.004197    -1.241268  0.012833\n",
      "LSQR+, L1($u$)           -0.067426  0.005261    -0.951209  0.008656\n",
      "LSQR+, L1($z$)           -0.050759  0.003698    -0.486623  0.005285\n",
      "LSQR+, L1($u$), L1($z$)  -0.073036  0.003498    -0.388167  0.005591\n",
      "agreement score only for mulimodal\n",
      "                        val error*           train error*          \n",
      "                              mean       std         mean       std\n",
      "init and reg.                                                      \n",
      "normal noise             -0.041706  0.014977    -1.844500  0.108128\n",
      "LSQR                     -0.033476  0.010215    -0.960839  0.052054\n",
      "LSQR+                    -0.034848  0.009337    -2.021890  0.130877\n",
      "LSQR+, L1($u$)           -0.056211  0.010180    -1.352383  0.057735\n",
      "LSQR+, L1($z$)           -0.015368  0.009418    -1.129102  0.082210\n",
      "LSQR+, L1($u$), L1($z$)  -0.048706  0.017285    -0.900661  0.047136\n",
      "agreement score only for mulimodal\n",
      "                        val error*           train error*          \n",
      "                              mean       std         mean       std\n",
      "init and reg.                                                      \n",
      "normal noise             -0.189840  0.045973    -0.470686  0.056957\n",
      "LSQR                     -0.159029  0.056122    -0.217531  0.039747\n",
      "LSQR+                    -0.278039  0.070936    -0.517003  0.129769\n",
      "LSQR+, L1($u$)           -0.269020  0.120913    -0.361660  0.192981\n",
      "LSQR+, L1($z$)           -0.101472  0.043197    -0.164637  0.040878\n",
      "LSQR+, L1($u$), L1($z$)  -0.097695  0.030504    -0.133306  0.034431\n",
      "agreement score only for mulimodal\n"
     ]
    }
   ],
   "source": [
    "for graph_name in graph_names:\n",
    "    init_improvement(results[graph_name], graph_name)\n",
    "    agreement_score(best_results[graph_name], graph_name)\n",
    "#     g_val = plot_boxes(best_results[graph_name], graph_name, \"val\")\n",
    "#     g_val = plot_boxes(best_results[graph_name], graph_name, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_order = [\"gated\", \"grad\", \"f.e.+dnn2\", \"n2v+dnn2\", \"Kumar et.al.\"]\n",
    "for split in [\"val\", \"train\"]:\n",
    "    all_data = []\n",
    "    for graph_name in graph_names:\n",
    "        data = make_comparison(best_results[graph_name], score=\"median_mag_error\", split=split)\n",
    "        all_data.append(\n",
    "                data.groupby(\"models\").agg({\"median_mag_error\": [\"mean\", \"std\"]}).rename(columns={\"median_mag_error\": graph_name})\n",
    "        )\n",
    "    result_tbl = pd.concat(all_data, axis=\"columns\").rename(index=model_dict_wo_enter).loc[model_order, :]\n",
    "    result_tbl.to_latex(os.path.join(tables_save_folder, f\"{split}_performance.tex\"), \n",
    "                                            escape=False, float_format=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
